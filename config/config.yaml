defaults:
  - model: wav2vec2
  - datasets:  # 1283k samples in total
    - nst_da  # 183k samples
    - nota  # 98.6k samples
    - ftspeech  # 996k samples
    - fleurs_da  # 2.5k samples
    - common_voice_13_da  # 2.75 samples
  - override hydra/job_logging: custom
  - _self_

dirs:
  data: data
  raw: raw
  processed: processed
  final: final
  models: models

seed: 4242

# Dataset parameters
characters_to_keep: 'abcdefghijklmnopqrstuvwxyzæøå0123456789éü '
dataset_probabilities:  # null = equal probability to every dataset
  train:
    - 0.20
    - 0.20
    - 0.20
    - 0.05
    - 0.05
  val: null
  test: null

# Model parameters
pipeline_id: ${model.name}-finetuned
hub_id: alexandrainst/${pipeline_id}
model_dir: ${dirs.models}/${pipeline_id}
push_to_hub: false
fp16: true

# Training parameters
wandb: false
wandb_project: CoRal
wandb_group: default
wandb_name: null
resume_from_checkpoint: false
ignore_data_skip: false
save_total_limit: 2

# Optimisation parameters
learning_rate: 3e-5
adam_first_momentum: 0.9
adam_second_momentum: 0.98
batch_size: 8
gradient_accumulation: 32
max_steps: 50_000
warmup_steps: 1_000
logging_steps: 10
eval_steps: 100
save_steps: 100
early_stopping: false
early_stopping_patience: 50
