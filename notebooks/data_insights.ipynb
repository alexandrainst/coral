{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from coral.Geography_Helper import Geography_Helper\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tables_from_db(\n",
    "    db_path: Path, tables: list[str] = [\"Recordings\", \"Speakers\"]\n",
    ") -> tuple[pd.DataFrame, ...]:\n",
    "    \"\"\"Load specified tables from a SQLite database and return them as a tuple of data frames.\n",
    "\n",
    "    Args:\n",
    "        db_path: The path to the SQLite database file.\n",
    "        tables: A list of table names to load. Defaults to [\"Recordings\", \"Speakers\"].\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing pandas DataFrames for the specified tables.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    dataframes = []\n",
    "    for table in tables:\n",
    "        query = f\"SELECT * FROM {table}\"\n",
    "        dataframe = pd.read_sql_query(query, conn)\n",
    "        dataframes.append(dataframe)\n",
    "    conn.close()\n",
    "    return tuple(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"internal\"\n",
    "\n",
    "dataset_name = \"CoRal-project/coral-v2\"\n",
    "\n",
    "\n",
    "# Define the path to the public database\n",
    "path_database_public = Path(\"/Volumes/CoRal/raw\") / \"CoRal_public.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset name is \"internal\"\n",
    "if dataset_name == \"internal\":\n",
    "    # Load the \"Recordings\" and \"Conversations\" tables from the database\n",
    "    df_recordings, df_conversations = load_tables_from_db(\n",
    "        path_database_public, [\"Recordings\", \"Conversations\"]\n",
    "    )\n",
    "\n",
    "    # Expand the conversations table to create two rows for each conversation, one for each speaker\n",
    "    df_conversation_expanded = pd.concat(\n",
    "        [\n",
    "            df_conversations.assign(\n",
    "                id_speaker=df_conversations[\"id_speaker_a\"]\n",
    "            ),  # Assign speaker A\n",
    "            df_conversations.assign(\n",
    "                id_speaker=df_conversations[\"id_speaker_b\"]\n",
    "            ),  # Assign speaker B\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    # Drop columns specific to the original conversation structure\n",
    "    df_conversation_expanded = df_conversation_expanded.drop(\n",
    "        columns=[\"id_speaker_a\", \"id_speaker_b\", \"id_recorder\"]\n",
    "    )\n",
    "\n",
    "    # Combine the recordings and expanded conversations into a single DataFrame\n",
    "    df_combined = pd.concat(\n",
    "        [df_recordings, df_conversation_expanded], ignore_index=True\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Handle huggingface datasets\n",
    "    if dataset_name == \"CoRal-project/coral\":\n",
    "        subsets = None  # No subsets for this dataset\n",
    "    elif dataset_name == \"CoRal-project/coral-v2\":\n",
    "        subsets = [\"read_aloud\", \"conversation\"]  # Define subsets for coral-v2\n",
    "\n",
    "    splits = [\"train\", \"val\", \"test\"]  # Define dataset splits\n",
    "\n",
    "    # Initialize an empty DataFrame to store combined data\n",
    "    df_combined = pd.DataFrame()\n",
    "\n",
    "    for subset in subsets:\n",
    "        # Load the dataset for the current subset\n",
    "        dataset = load_dataset(dataset_name, subset)\n",
    "        for split in splits:\n",
    "            # Remove the \"audio\" column and convert the split to a pandas DataFrame\n",
    "            ds_split = dataset[split].remove_columns([\"audio\"])\n",
    "            df = ds_split.to_pandas()\n",
    "\n",
    "            # Add metadata columns for split and subset\n",
    "            df[\"split\"] = split\n",
    "            df[\"subset\"] = subset\n",
    "\n",
    "            # Append the data to the combined DataFrame\n",
    "            if df_combined.empty:\n",
    "                df_combined = df\n",
    "            else:\n",
    "                df_combined = pd.concat([df_combined, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"id\"] = df_combined[\"id_recording\"].combine_first(\n",
    "    df_combined[\"id_conversation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_speakers = load_tables_from_db(path_database_public, [\"Speakers\"])[0]\n",
    "\n",
    "# Create a mapping from df_speakers for age, gender, and dialect\n",
    "speaker_map = df_speakers.set_index(\"id_speaker\")[\n",
    "    [\"age\", \"gender\", \"dialect\", \"zip_school\"]\n",
    "].to_dict(orient=\"index\")\n",
    "speaker_map = df_speakers.rename(columns={\"zip_school\": \"zipcode\"})\n",
    "\n",
    "\n",
    "# BEGIN: Create individual mapping dictionaries\n",
    "age_map = df_speakers.set_index(\"id_speaker\")[\"age\"].to_dict()\n",
    "gender_map = df_speakers.set_index(\"id_speaker\")[\"gender\"].to_dict()\n",
    "dialect_map = df_speakers.set_index(\"id_speaker\")[\"dialect\"].to_dict()\n",
    "zipcode_map = df_speakers.set_index(\"id_speaker\")[\"zip_school\"].to_dict()\n",
    "# END: Create individual mapping dictionaries\n",
    "\n",
    "# BEGIN: Apply individual mapping dictionaries to df_combined\n",
    "df_combined[\"age\"] = df_combined[\"id_speaker\"].map(age_map)\n",
    "df_combined[\"gender\"] = df_combined[\"id_speaker\"].map(gender_map)\n",
    "df_combined[\"dialect\"] = df_combined[\"id_speaker\"].map(dialect_map)\n",
    "df_combined[\"zipcode\"] = df_combined[\"id_speaker\"].map(zipcode_map)\n",
    "# END: Apply individual mapping dictionaries to df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration = load_tables_from_db(path_database_public, [\"Durations\"])[0]\n",
    "\n",
    "duration_map = df_duration.set_index(\"id\")[\"duration\"].to_dict()\n",
    "# Apply the duration mapping to df_combined\n",
    "df_combined[\"duration_seconds\"] = df_combined[\"id\"].map(duration_map)\n",
    "df_combined[\"duration_minutes\"] = df_combined[\"duration_seconds\"] / 60\n",
    "df_combined[\"duration_hours\"] = df_combined[\"duration_seconds\"] / 3600\n",
    "\n",
    "# Set duration to 0 for all duplicates based on the 'id' column\n",
    "df_combined.loc[\n",
    "    df_combined.duplicated(subset=\"id\", keep=\"first\"),\n",
    "    [\"duration_seconds\", \"duration_minutes\", \"duration_hours\"],\n",
    "] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"type\"] = df_combined[\"id\"].apply(\n",
    "    lambda x: \"conversation\" if \"conv\" in x else \"read_aloud\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range for binning\n",
    "bin_edges = [0, 25, 50, 200]  # Bins for age ranges: 0-24, 25-50, 50+\n",
    "\n",
    "# Add a new column for binned data\n",
    "df_combined[\"age_binned\"] = pd.cut(\n",
    "    df_combined[\"age\"], bins=bin_edges, right=False, labels=[\"0-25\", \"25-50\", \"50+\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_map = {\n",
    "    # Bornholmsk\n",
    "    \"bornholmsk\": \"Bornholmsk\",\n",
    "    # Fynsk\n",
    "    \"fynsk\": \"Fynsk\",\n",
    "    \"√∏stfynsk\": \"Fynsk\",\n",
    "    \"vestfynsk (nordvest-, sydvestfynsk)\": \"Fynsk\",\n",
    "    \"sydfynsk\": \"Fynsk\",\n",
    "    \"langelandsk\": \"Fynsk\",\n",
    "    \"t√•singsk (m. thur√∏)\": \"Fynsk\",\n",
    "    \"√¶r√∏sk (m. ly√∏, avernak√∏, stryn√∏, birkholm, drej√∏)\": \"Fynsk\",\n",
    "    # K√∏benhavnsk\n",
    "    \"amagerm√•l\": \"K√∏benhavnsk\",\n",
    "    \"k√∏benhavnsk\": \"K√∏benhavnsk\",\n",
    "    # Sj√¶llandsk\n",
    "    \"sj√¶llandsk\": \"Sj√¶llandsk\",\n",
    "    \"nordsj√¶llandsk\": \"Sj√¶llandsk\",\n",
    "    \"√∏stsj√¶llandsk\": \"Sj√¶llandsk\",\n",
    "    \"nordvestsj√¶llandsk\": \"Sj√¶llandsk\",\n",
    "    \"sydvestsj√¶llandsk\": \"Sj√¶llandsk\",\n",
    "    \"sydsj√¶llandsk (sydligt sydsj√¶llandsk)\": \"Sj√¶llandsk\",\n",
    "    # Syd√∏m√•l\n",
    "    \"√∏stm√∏nsk\": \"Syd√∏m√•l\",\n",
    "    \"vestm√∏nsk\": \"Syd√∏m√•l\",\n",
    "    \"nordfalstersk\": \"Syd√∏m√•l\",\n",
    "    \"sydfalstersk\": \"Syd√∏m√•l\",\n",
    "    \"lollandsk\": \"Syd√∏m√•l\",\n",
    "    \"syd√∏m√•l\": \"Syd√∏m√•l\",\n",
    "    # S√∏nderjysk\n",
    "    \"s√∏nderjysk\": \"S√∏nderjysk\",\n",
    "    \"√∏stligt s√∏nderjysk (m. als)\": \"S√∏nderjysk\",\n",
    "    \"vestlig s√∏nderjysk (m. mand√∏ og r√∏m√∏)\": \"S√∏nderjysk\",\n",
    "    \"syd for rigsgr√¶nsen: mellemslesvisk, angelm√•l, fjoldem√•l\": \"S√∏nderjysk\",\n",
    "    \"mellemslesvisk\": \"S√∏nderjysk\",\n",
    "    # Vestjysk\n",
    "    \"vestjysk\": \"Vestjysk\",\n",
    "    \"thybom√•l\": \"Vestjysk\",\n",
    "    \"morsingm√•l\": \"Vestjysk\",\n",
    "    \"sallingm√•l\": \"Vestjysk\",\n",
    "    \"hardsysselsk\": \"Vestjysk\",\n",
    "    \"fjandbom√•l\": \"Vestjysk\",\n",
    "    \"sydvestjysk (m. fan√∏)\": \"Vestjysk\",\n",
    "    \"syd√∏stjysk\": \"Vestjysk\",\n",
    "    # √òstjysk\n",
    "    \"midt√∏stjysk\": \"√òstjysk\",\n",
    "    \"ommersysselsk\": \"√òstjysk\",\n",
    "    \"djurslandsk (nord-, syddjurs m. nord- og sydsams√∏, anholt)\": \"√òstjysk\",\n",
    "    \"√∏stjysk\": \"√òstjysk\",\n",
    "    # Nordjysk (NEW)\n",
    "    \"n√∏rrejysk\": \"Nordjysk\",\n",
    "    \"vendsysselsk (m. hanherred og l√¶s√∏)\": \"Nordjysk\",\n",
    "    \"himmerlandsk\": \"Nordjysk\",\n",
    "    # General fallback (optional)\n",
    "    \"jysk\": \"Jysk\",\n",
    "}\n",
    "\n",
    "df_combined[\"dialect\"] = df_combined[\"dialect\"].apply(lambda x: dialect_map.get(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_helper = Geography_Helper()\n",
    "df_combined[\"kommunekod\"] = df_combined[\"zipcode\"].apply(\n",
    "    lambda x: geo_helper.getMunicipality(x)\n",
    ")\n",
    "df_combined[\"regionskod\"] = df_combined[\"kommunekod\"].apply(\n",
    "    lambda x: geo_helper.getRegion(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def distribution_plot(\n",
    "    df: pd.DataFrame,\n",
    "    group_by: str,\n",
    "    agg_col: str = None,\n",
    "    agg_func: str = \"size\",  # 'size', 'sum', 'nunique', 'percentage'\n",
    "    stack_col: str = None,\n",
    "    figsize: tuple = (10, 6),\n",
    "    title: str = \"Distribution Plot\",\n",
    "    colormap: str = \"tab10\",\n",
    "    xlabel: str = None,\n",
    "    ylabel: str = None,\n",
    "    save_dir: str = None,\n",
    "):\n",
    "    \"\"\"Create a (stacked) bar plot from a DataFrame with value labels and total labels.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        group_by (str): Column to group by on the x-axis.\n",
    "        agg_col (str): Column to aggregate if using 'sum' or 'nunique'. Not needed for 'size'.\n",
    "        agg_func (str): Aggregation function: 'size', 'nunique', 'sum', or 'percentage'.\n",
    "        stack_col (str, optional): Column to stack bars by (categorical).\n",
    "        figsize (tuple): Figure size.\n",
    "        title (str): Plot title.\n",
    "        colormap (str): Matplotlib colormap name.\n",
    "        xlabel (str): Custom label for x-axis.\n",
    "        ylabel (str): Custom label for y-axis.\n",
    "        save_dir (str): Directory to save the plot as PNG if provided.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.axes.Axes: The plot axes.\n",
    "    \"\"\"\n",
    "    valid_funcs = [\"size\", \"sum\", \"nunique\", \"percentage\"]\n",
    "    if agg_func not in valid_funcs:\n",
    "        raise ValueError(f\"agg_func must be one of {valid_funcs}\")\n",
    "    if agg_func in [\"sum\", \"nunique\"] and agg_col is None:\n",
    "        raise ValueError(f\"agg_col must be specified when using '{agg_func}'\")\n",
    "\n",
    "    # Group and aggregate\n",
    "    if stack_col:\n",
    "        if agg_func == \"size\" or agg_func == \"percentage\":\n",
    "            data = df.groupby([group_by, stack_col]).size().unstack(fill_value=0)\n",
    "        elif agg_func == \"sum\":\n",
    "            data = (\n",
    "                df.groupby([group_by, stack_col])[agg_col].sum().unstack(fill_value=0)\n",
    "            )\n",
    "        elif agg_func == \"nunique\":\n",
    "            data = (\n",
    "                df.groupby([group_by, stack_col])[agg_col]\n",
    "                .nunique()\n",
    "                .unstack(fill_value=0)\n",
    "            )\n",
    "    else:\n",
    "        if agg_func == \"size\" or agg_func == \"percentage\":\n",
    "            data = df[group_by].value_counts().sort_index()\n",
    "        elif agg_func == \"sum\":\n",
    "            data = df.groupby(group_by)[agg_col].sum()\n",
    "        elif agg_func == \"nunique\":\n",
    "            data = df.groupby(group_by)[agg_col].nunique()\n",
    "\n",
    "    # Convert to percentage if needed\n",
    "    if agg_func == \"percentage\":\n",
    "        if stack_col:\n",
    "            data = data.div(data.sum(axis=1), axis=0) * 100\n",
    "        else:\n",
    "            data = (data / data.sum()) * 100\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=figsize)\n",
    "    if stack_col:\n",
    "        ax = data.plot(kind=\"bar\", stacked=True, figsize=figsize, colormap=colormap)\n",
    "        for idx, group in enumerate(data.index):\n",
    "            y_offset = 0\n",
    "            for col in data.columns:\n",
    "                value = data.loc[group, col]\n",
    "                if value > 0:\n",
    "                    label = (\n",
    "                        f\"{value:.2f}%\" if agg_func == \"percentage\" else f\"{value:.2f}\"\n",
    "                    )\n",
    "                    ax.text(\n",
    "                        idx,\n",
    "                        y_offset + value / 2,\n",
    "                        label,\n",
    "                        ha=\"center\",\n",
    "                        va=\"center\",\n",
    "                        fontsize=9,\n",
    "                    )\n",
    "                    y_offset += value\n",
    "            total = data.loc[group].sum()\n",
    "            total_label = (\n",
    "                f\"{total:.2f}%\" if agg_func == \"percentage\" else f\"{total:.2f}\"\n",
    "            )\n",
    "            ax.text(\n",
    "                idx,\n",
    "                y_offset + max(data.values.max() * 0.01, 1),\n",
    "                total_label,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=10,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "    else:\n",
    "        ax = data.plot(kind=\"bar\", figsize=figsize, colormap=colormap)\n",
    "        for idx, value in enumerate(data.values):\n",
    "            label = f\"{value:.2f}%\" if agg_func == \"percentage\" else f\"{value:.2f}\"\n",
    "            ax.text(\n",
    "                idx,\n",
    "                value + max(data.max() * 0.01, 1),\n",
    "                label,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=10,\n",
    "            )\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel if xlabel else group_by)\n",
    "    ax.set_ylabel(\n",
    "        ylabel\n",
    "        if ylabel\n",
    "        else (\n",
    "            \"Percentage\"\n",
    "            if agg_func == \"percentage\"\n",
    "            else (\"Count\" if agg_func == \"size\" else agg_func.capitalize())\n",
    "        )\n",
    "    )\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if save_dir is provided\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        filename = os.path.join(save_dir, f\"{title.replace(' ', '_')}.png\")\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "        print(f\"Plot saved to: {filename}\")\n",
    "\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_geo_counts(\n",
    "    df,\n",
    "    group_by: str,\n",
    "    geo_level: str,\n",
    "    agg_col: str = None,\n",
    "    agg_func: str = \"size\",\n",
    "    rename_col: str = None,\n",
    "    title: str = None,\n",
    "    save_dir: str = None,\n",
    "    interactive: bool = False,\n",
    "    cmap: str = \"viridis\",\n",
    "    round_decimals: int = 2,\n",
    "    **explore_kwargs,\n",
    "):\n",
    "    \"\"\"Plots a geographic distribution (static or interactive choropleth).\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input data.\n",
    "        group_by (str): Column to group by (should match a column in geo data).\n",
    "        geo_level (str): Level of geography (used with geo_helper.get_dfmap()).\n",
    "        agg_col (str): Column to aggregate on (required for 'sum' and 'nunique').\n",
    "        agg_func (str): Aggregation function: 'size', 'sum', or 'nunique'.\n",
    "        rename_col (str): Optionally rename group_by to match geo data.\n",
    "        title (str): Title for the plot.\n",
    "        save_dir (str): If provided, saves static plot to this directory.\n",
    "        interactive (bool): Use interactive map if True.\n",
    "        cmap (str): Colormap for the plot.\n",
    "        round_decimals (int): Rounding of numeric output.\n",
    "        **explore_kwargs: Extra args for GeoDataFrame.explore() if interactive.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: Merged and aggregated geodata.\n",
    "    \"\"\"\n",
    "    # Determine default name for value_col\n",
    "    if agg_func == \"size\":\n",
    "        value_col = \"count\"\n",
    "        data = df.groupby(group_by).size().reset_index(name=value_col)\n",
    "    elif agg_func == \"nunique\":\n",
    "        if not agg_col:\n",
    "            raise ValueError(\"agg_col must be specified for 'nunique'\")\n",
    "        value_col = f\"n_unique_{agg_col}\"\n",
    "        data = df.groupby(group_by)[agg_col].nunique().reset_index(name=value_col)\n",
    "    elif agg_func == \"sum\":\n",
    "        if not agg_col:\n",
    "            raise ValueError(\"agg_col must be specified for 'sum'\")\n",
    "        value_col = f\"sum_{agg_col}\"\n",
    "        data = df.groupby(group_by)[agg_col].sum().reset_index(name=value_col)\n",
    "    else:\n",
    "        raise ValueError(\"agg_func must be one of: 'size', 'sum', 'nunique'\")\n",
    "\n",
    "    # Rename group_by column if needed\n",
    "    if rename_col:\n",
    "        data = data.rename(columns={group_by: rename_col})\n",
    "        group_by = rename_col\n",
    "\n",
    "    # Merge with geo data\n",
    "    dfmap = geo_helper.get_dfmap(geo_level)\n",
    "    dfmap = pd.merge(dfmap, data, how=\"left\", on=group_by)\n",
    "    dfmap[value_col] = dfmap[value_col].fillna(0).round(round_decimals)\n",
    "\n",
    "    # Plotting\n",
    "    if interactive:\n",
    "        if title:\n",
    "            print(f\"üó∫Ô∏è {title}\")\n",
    "        return dfmap.explore(\n",
    "            column=value_col,\n",
    "            cmap=cmap,\n",
    "            legend=True,\n",
    "            tooltip=[group_by, value_col],\n",
    "            **explore_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        dfmap.plot(column=value_col, cmap=cmap, legend=True)\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "        if save_dir:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            filepath = os.path.join(save_dir, f\"{title.replace(' ', '_')}.png\")\n",
    "            plt.savefig(filepath, bbox_inches=\"tight\")\n",
    "            print(f\"Map saved to: {filepath}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return dfmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8, 5)  # Set default plot size to 10x6 inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_combined.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"age_binned\",\n",
    "    agg_func=\"size\",\n",
    "    title=\"Age Distribution (Samples)\",\n",
    "    xlabel=\"Age Group\",\n",
    "    ylabel=\"Number of Samples\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"age_binned\",\n",
    "    agg_func=\"percentage\",\n",
    "    title=\"Age Distribution (Percentage of Samples)\",\n",
    "    xlabel=\"Age Group\",\n",
    "    ylabel=\"Percentage of Samples\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"age_binned\",\n",
    "    agg_func=\"nunique\",\n",
    "    agg_col=\"id_speaker\",\n",
    "    title=\"Age Distribution (Speakers)\",\n",
    "    xlabel=\"Age Group\",\n",
    "    ylabel=\"Number of Unique Speakers\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"age_binned\",\n",
    "    agg_func=\"sum\",\n",
    "    agg_col=\"duration_hours\",\n",
    "    title=\"Age Distribution (Hours of Recordings)\",\n",
    "    xlabel=\"Age Group\",\n",
    "    ylabel=\"Total Duration (Hours)\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialect distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"dialect\",\n",
    "    agg_func=\"size\",\n",
    "    title=\"Dialect Distribution (Samples)\",\n",
    "    xlabel=\"Dialect\",\n",
    "    ylabel=\"Number of Samples\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"dialect\",\n",
    "    agg_func=\"percentage\",\n",
    "    title=\"Dialect Distribution (Percentage of Samples)\",\n",
    "    xlabel=\"Dialect\",\n",
    "    ylabel=\"Percentage of Samples\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"dialect\",\n",
    "    agg_func=\"nunique\",\n",
    "    agg_col=\"id_speaker\",\n",
    "    title=\"Dialect Distribution (Speakers)\",\n",
    "    xlabel=\"Dialect\",\n",
    "    ylabel=\"Number of Unique Speakers\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"dialect\",\n",
    "    agg_func=\"sum\",\n",
    "    agg_col=\"duration_hours\",\n",
    "    title=\"Dialect Distribution (Hours of Recordings)\",\n",
    "    xlabel=\"Dialect\",\n",
    "    ylabel=\"Total Duration (Hours)\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"gender\",\n",
    "    agg_func=\"size\",\n",
    "    title=\"Gender Distribution (Samples)\",\n",
    "    xlabel=\"Gender\",\n",
    "    ylabel=\"Number of Samples\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"gender\",\n",
    "    agg_func=\"percentage\",\n",
    "    title=\"Gender Distribution (Percentage of Samples)\",\n",
    "    xlabel=\"Gender\",\n",
    "    ylabel=\"Percentage of Samples\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"gender\",\n",
    "    agg_func=\"nunique\",\n",
    "    agg_col=\"id_speaker\",\n",
    "    title=\"Gender Distribution (Speakers)\",\n",
    "    xlabel=\"Gender\",\n",
    "    ylabel=\"Number of Unique Speakers\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "distribution_plot(\n",
    "    df,\n",
    "    group_by=\"gender\",\n",
    "    agg_func=\"sum\",\n",
    "    agg_col=\"duration_hours\",\n",
    "    title=\"Gender Distribution (Hours of Recordings)\",\n",
    "    xlabel=\"Gender\",\n",
    "    ylabel=\"Total Duration (Hours)\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geoplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_geo_counts(\n",
    "    df,\n",
    "    group_by=\"zipcode\",\n",
    "    geo_level=\"zipcode\",\n",
    "    rename_col=\"postnummer\",\n",
    "    agg_func=\"size\",\n",
    "    title=\"Zipcode Distribution (Samples)\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "plot_geo_counts(\n",
    "    df,\n",
    "    group_by=\"zipcode\",\n",
    "    geo_level=\"zipcode\",\n",
    "    rename_col=\"postnummer\",\n",
    "    agg_func=\"nunique\",\n",
    "    agg_col=\"id_speaker\",\n",
    "    title=\"Zipcode Distribution (Speakers)\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "plot_geo_counts(\n",
    "    df,\n",
    "    group_by=\"zipcode\",\n",
    "    geo_level=\"zipcode\",\n",
    "    rename_col=\"postnummer\",\n",
    "    agg_func=\"sum\",\n",
    "    agg_col=\"duration_hours\",\n",
    "    title=\"Zipcode Distribution (Hours of Recordings)\",\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of recordings by municipality\n",
    "plot_geo_counts(\n",
    "    df,\n",
    "    group_by=\"kommunekod\",\n",
    "    geo_level=\"municipality\",\n",
    "    agg_func=\"size\",\n",
    "    value_col=\"count\",\n",
    "    title=\"Municipality Distribution (Recordings)\",\n",
    "    interactive=False,\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "plot_geo_counts(\n",
    "    df,\n",
    "    group_by=\"kommunekod\",\n",
    "    geo_level=\"municipality\",\n",
    "    agg_func=\"nunique\",\n",
    "    agg_col=\"id_speaker\",\n",
    "    value_col=\"count\",\n",
    "    title=\"Municipality Distribution (Unique Speakers)\",\n",
    "    interactive=False,\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "plot_geo_counts(\n",
    "    df,\n",
    "    group_by=\"kommunekod\",\n",
    "    geo_level=\"municipality\",\n",
    "    agg_func=\"sum\",\n",
    "    agg_col=\"duration_hours\",\n",
    "    value_col=\"count\",\n",
    "    title=\"Municipality Distribution (Total Duration in Hours)\",\n",
    "    interactive=False,\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_geo_counts(\n",
    "    df,\n",
    "    group_by=\"regionskod\",\n",
    "    geo_level=\"region\",\n",
    "    agg_func=\"size\",\n",
    "    title=\"Region Distribution (Recordings)\",\n",
    "    interactive=False,\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "plot_geo_counts(\n",
    "    df,\n",
    "    group_by=\"regionskod\",\n",
    "    geo_level=\"region\",\n",
    "    agg_func=\"nunique\",\n",
    "    agg_col=\"id_speaker\",\n",
    "    title=\"Region Distribution (Unique Speakers)\",\n",
    "    interactive=False,\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")\n",
    "\n",
    "plot_geo_counts(\n",
    "    df,\n",
    "    group_by=\"regionskod\",\n",
    "    geo_level=\"region\",\n",
    "    agg_func=\"sum\",\n",
    "    agg_col=\"duration_hours\",\n",
    "    title=\"Region Distribution (Total Duration in Hours)\",\n",
    "    interactive=False,\n",
    "    save_dir=\"outputs/visualizations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_geo_counts(\n",
    "    df,\n",
    "    group_by=\"kommunekod\",\n",
    "    geo_level=\"municipality\",\n",
    "    agg_func=\"size\",\n",
    "    title=\"Municipality Distribution (Recordings)\",\n",
    "    interactive=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.groupby(\"type\")[\"duration_hours\"].sum().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"duration_hours\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"id_speaker\"].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "539a6cd0f424a0714504c82e57dfed0db3a49f99005862d5e6904d2d478bf7ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
